{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP를 통한 External Database 활용\n",
    "\n",
    "이 노트북에서는 Strands Agents와 MCP (Model Context Protocol)를 사용하여 외부 데이터베이스인 Arxiv, ChEMBL, PubMed, ClinicalTrials.gov를 연동하는 방법을 실습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- MCP를 통해 외부 데이터베이스를 Agent의 도구로 활용하는 방법 이해\n",
    "- Agent-as-tool 패턴을 사용한 다중 MCP 서버 통합\n",
    "- 실제 연구 질문에 대한 답변 생성 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정\n",
    "\n",
    "필요한 라이브러리와 의존성을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "%pip install strands-agents strands-agents-tools mcp boto3 arxiv chembl-webresource-client python-dateutil pubmedmcp --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import sys\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS SDK\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Strands Agents\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from strands.tools.mcp import MCPClient\n",
    "\n",
    "# MCP\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"external_dbs_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCP 서버 구성\n",
    "\n",
    "각 외부 데이터베이스에 대한 MCP 서버를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_arxiv.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import arxiv\n",
    "from dateutil import parser\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"arxiv_mcp\")\n",
    "\n",
    "MAX_RESULTS = 10\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"arxiv_tools\")\n",
    "    logger.info(\"arXiv MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "def _is_within_date_range(\n",
    "    date: datetime, start: datetime | None, end: datetime | None\n",
    ") -> bool:\n",
    "    \"\"\"Check if a date falls within the specified range.\"\"\"\n",
    "    if start and not start.tzinfo:\n",
    "        start = start.replace(tzinfo=timezone.utc)\n",
    "    if end and not end.tzinfo:\n",
    "        end = end.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    if start and date < start:\n",
    "        return False\n",
    "    if end and date > end:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _process_paper(paper: arxiv.Result) -> Dict[str, Any]:\n",
    "    \"\"\"Process paper information with resource URI.\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get_short_id(),\n",
    "        \"title\": paper.title,\n",
    "        \"authors\": [author.name for author in paper.authors],\n",
    "        \"abstract\": paper.summary,\n",
    "        \"categories\": paper.categories,\n",
    "        \"published\": paper.published.isoformat(),\n",
    "        \"url\": paper.pdf_url,\n",
    "        \"resource_uri\": f\"arxiv://{paper.get_short_id()}\",\n",
    "    }\n",
    "\n",
    "@mcp.tool()\n",
    "async def search_papers(\n",
    "    query: str, \n",
    "    max_results: int = 10, \n",
    "    date_from: str = None, \n",
    "    date_to: str = None, \n",
    "    categories: List[str] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search for papers on arXiv with advanced filtering.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        max_results = min(int(max_results), MAX_RESULTS)\n",
    "\n",
    "        # Build search query with category filtering\n",
    "        if categories:\n",
    "            category_filter = \" OR \".join(f\"cat:{cat}\" for cat in categories)\n",
    "            query = f\"({query}) AND ({category_filter})\"\n",
    "\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        )\n",
    "\n",
    "        # Process results with date filtering\n",
    "        results = []\n",
    "        try:\n",
    "            date_from_obj = (\n",
    "                parser.parse(date_from).replace(tzinfo=timezone.utc)\n",
    "                if date_from\n",
    "                else None\n",
    "            )\n",
    "            date_to_obj = (\n",
    "                parser.parse(date_to).replace(tzinfo=timezone.utc) if date_to else None\n",
    "            )\n",
    "        except (ValueError, TypeError) as e:\n",
    "            return [{\"error\": f\"Invalid date format - {str(e)}\"}]\n",
    "\n",
    "        for paper in client.results(search):\n",
    "            if _is_within_date_range(paper.published, date_from_obj, date_to_obj):\n",
    "                results.append(_process_paper(paper))\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search error: {str(e)}\")\n",
    "        return [{\"error\": f\"Search failed: {str(e)}\"}]\n",
    "\n",
    "@mcp.tool()\n",
    "async def download_paper(paper_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Download a paper from arXiv.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        \n",
    "        for paper in client.results(search):\n",
    "            return {\n",
    "                \"id\": paper.get_short_id(),\n",
    "                \"title\": paper.title,\n",
    "                \"url\": paper.pdf_url,\n",
    "                \"download_status\": \"success\",\n",
    "                \"resource_uri\": f\"arxiv://{paper.get_short_id()}\"\n",
    "            }\n",
    "        \n",
    "        return {\"error\": f\"Paper with ID {paper_id} not found\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Download error: {str(e)}\")\n",
    "        return {\"error\": f\"Download failed: {str(e)}\"}\n",
    "\n",
    "@mcp.tool()\n",
    "async def read_paper(paper_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Read the content of an arXiv paper.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "\n",
    "        for paper in client.results(search):\n",
    "            return {\n",
    "                \"id\": paper.get_short_id(),\n",
    "                \"title\": paper.title,\n",
    "                \"authors\": [author.name for author in paper.authors],\n",
    "                \"abstract\": paper.summary,\n",
    "                \"categories\": paper.categories,\n",
    "                \"published\": paper.published.isoformat(),\n",
    "                \"content_type\": \"text\",\n",
    "                \"content\": paper.summary,\n",
    "            }\n",
    "\n",
    "        return {\"error\": f\"Paper with ID {paper_id} not found\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Read error: {str(e)}\")\n",
    "        return {\"error\": f\"Read failed: {str(e)}\"}\n",
    "\n",
    "@mcp.tool()\n",
    "async def list_papers(\n",
    "    category: str = None, max_results: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get a list of the latest papers in a specific category.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        max_results = min(int(max_results), MAX_RESULTS)\n",
    "\n",
    "        query = f\"cat:{category}\" if category else \"\"\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for paper in client.results(search):\n",
    "            results.append(_process_paper(paper))\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"List error: {str(e)}\")\n",
    "        return [{\"error\": f\"List failed: {str(e)}\"}]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_chembl.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "MAXIMUM_ACTIVITY = 100\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"chembl_mcp\")\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"chembl_tools\")\n",
    "    logger.info(\"ChEMBL MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def compount_activity(compound_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get activity data for the specified compound\"\"\"\n",
    "    client = new_client\n",
    "    molecule_id = client.molecule.filter(pref_name__iexact=compound_name).only('molecule_chembl_id')[0]\n",
    "    activity = list(client.activity.filter(molecule_chembl_id=molecule_id['molecule_chembl_id']).filter(standard_type=\"IC50\").only(['pchembl_value', 'assay_description', 'canonical_smiles']))\n",
    "    if len(activity) > MAXIMUM_ACTIVITY:\n",
    "        activity = activity[:MAXIMUM_ACTIVITY]\n",
    "    return activity\n",
    "\n",
    "@mcp.tool()\n",
    "async def target_activity(target_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get activity data for the specified target\"\"\"\n",
    "    client = new_client\n",
    "    target_id = client.target.filter(target_synonym__icontains=target_name, organism='Homo sapiens').only('target_chembl_id')[0]\n",
    "    activity = list(client.activity.filter(target_chembl_id=target_id['target_chembl_id']).filter(standard_type=\"IC50\").only(['pchembl_value', 'assay_description', 'canonical_smiles']))\n",
    "    if len(activity) > MAXIMUM_ACTIVITY:\n",
    "        activity = activity[:MAXIMUM_ACTIVITY]\n",
    "    return activity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_pubmed.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import httpx\n",
    "from defusedxml import ElementTree as ET\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"pubmed_mcp\")\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"pubmed_tools\")\n",
    "    logger.info(\"PubMed MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "# Helper functions for PubMed API\n",
    "def search_pubmed(query: str, max_results: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search PubMed for articles matching the query\"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "    \n",
    "    # Search for IDs\n",
    "    search_url = f\"{base_url}/esearch.fcgi\"\n",
    "    search_params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": query,\n",
    "        \"retmax\": max_results,\n",
    "        \"retmode\": \"json\",\n",
    "        \"sort\": \"relevance\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = httpx.get(search_url, params=search_params)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        # Extract IDs\n",
    "        id_list = search_data[\"esearchresult\"][\"idlist\"]\n",
    "        if not id_list:\n",
    "            return []\n",
    "        \n",
    "        # Fetch article details\n",
    "        fetch_url = f\"{base_url}/efetch.fcgi\"\n",
    "        fetch_params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"id\": \",\".join(id_list),\n",
    "            \"retmode\": \"xml\"\n",
    "        }\n",
    "        \n",
    "        fetch_response = httpx.get(fetch_url, params=fetch_params)\n",
    "        fetch_response.raise_for_status()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(fetch_response.text)\n",
    "        articles = []\n",
    "        \n",
    "        for article_element in root.findall(\".//PubmedArticle\"):\n",
    "            try:\n",
    "                article = {}\n",
    "                \n",
    "                # Extract PMID\n",
    "                pmid = article_element.find(\".//PMID\")\n",
    "                if pmid is not None:\n",
    "                    article[\"id\"] = pmid.text\n",
    "                \n",
    "                # Extract title\n",
    "                title = article_element.find(\".//ArticleTitle\")\n",
    "                if title is not None:\n",
    "                    article[\"title\"] = title.text\n",
    "                \n",
    "                # Extract abstract\n",
    "                abstract_parts = article_element.findall(\".//AbstractText\")\n",
    "                if abstract_parts:\n",
    "                    abstract = \" \".join([part.text for part in abstract_parts if part.text])\n",
    "                    article[\"abstract\"] = abstract\n",
    "                \n",
    "                # Extract authors\n",
    "                author_elements = article_element.findall(\".//Author\")\n",
    "                if author_elements:\n",
    "                    authors = []\n",
    "                    for author in author_elements:\n",
    "                        last_name = author.find(\"LastName\")\n",
    "                        fore_name = author.find(\"ForeName\")\n",
    "                        if last_name is not None and fore_name is not None:\n",
    "                            authors.append(f\"{fore_name.text} {last_name.text}\")\n",
    "                        elif last_name is not None:\n",
    "                            authors.append(last_name.text)\n",
    "                    article[\"authors\"] = \", \".join(authors)\n",
    "                \n",
    "                # Extract journal info\n",
    "                journal = article_element.find(\".//Journal/Title\")\n",
    "                if journal is not None:\n",
    "                    article[\"journal\"] = journal.text\n",
    "                \n",
    "                # Extract publication year\n",
    "                pub_date = article_element.find(\".//PubDate/Year\")\n",
    "                if pub_date is not None:\n",
    "                    article[\"year\"] = pub_date.text\n",
    "                \n",
    "                articles.append(article)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching PubMed: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_pubmed_article_details(pmid: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Get detailed information about a specific PubMed article\"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "    fetch_url = f\"{base_url}/efetch.fcgi\"\n",
    "    \n",
    "    fetch_params = {\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"xml\"}\n",
    "    \n",
    "    try:\n",
    "        fetch_response = httpx.get(fetch_url, params=fetch_params)\n",
    "        fetch_response.raise_for_status()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(fetch_response.text)\n",
    "        article_element = root.find(\".//PubmedArticle\")\n",
    "        \n",
    "        if article_element is None:\n",
    "            return None\n",
    "        \n",
    "        article = {\"id\": pmid, \"references\": []}\n",
    "        \n",
    "        # Extract title\n",
    "        title = article_element.find(\".//ArticleTitle\")\n",
    "        if title is not None:\n",
    "            article[\"title\"] = title.text\n",
    "        \n",
    "        # Extract abstract\n",
    "        abstract_parts = article_element.findall(\".//AbstractText\")\n",
    "        if abstract_parts:\n",
    "            abstract = \" \".join([part.text for part in abstract_parts if part.text])\n",
    "            article[\"abstract\"] = abstract\n",
    "        \n",
    "        # Extract authors\n",
    "        author_elements = article_element.findall(\".//Author\")\n",
    "        if author_elements:\n",
    "            authors = []\n",
    "            for author in author_elements:\n",
    "                last_name = author.find(\"LastName\")\n",
    "                fore_name = author.find(\"ForeName\")\n",
    "                if last_name is not None and fore_name is not None:\n",
    "                    authors.append(f\"{fore_name.text} {last_name.text}\")\n",
    "                elif last_name is not None:\n",
    "                    authors.append(last_name.text)\n",
    "            article[\"authors\"] = \", \".join(authors)\n",
    "        \n",
    "        # Extract journal info\n",
    "        journal = article_element.find(\".//Journal/Title\")\n",
    "        if journal is not None:\n",
    "            article[\"journal\"] = journal.text\n",
    "        \n",
    "        # Extract publication year\n",
    "        pub_date = article_element.find(\".//PubDate/Year\")\n",
    "        if pub_date is not None:\n",
    "            article[\"year\"] = pub_date.text\n",
    "        \n",
    "        # Extract DOI\n",
    "        article_id_list = article_element.findall(\".//ArticleId\")\n",
    "        for article_id in article_id_list:\n",
    "            if article_id.get(\"IdType\") == \"doi\":\n",
    "                article[\"doi\"] = article_id.text\n",
    "        \n",
    "        # Extract keywords\n",
    "        keyword_elements = article_element.findall(\".//Keyword\")\n",
    "        if keyword_elements:\n",
    "            keywords = [k.text for k in keyword_elements if k.text]\n",
    "            article[\"keywords\"] = \", \".join(keywords)\n",
    "        \n",
    "        return article\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching article details: {e}\")\n",
    "        return None\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search(query: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles matching the query.\"\"\"\n",
    "    logger.info(f\"Searching PubMed for: {query}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results\")\n",
    "    return results\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_get_article(pmid: str):\n",
    "    \"\"\"Get detailed information about a specific PubMed article.\"\"\"\n",
    "    logger.info(f\"Fetching PubMed article: {pmid}\")\n",
    "    result = get_pubmed_article_details(pmid)\n",
    "    if result:\n",
    "        logger.info(f\"Successfully fetched article: {pmid}\")\n",
    "    else:\n",
    "        logger.info(f\"Failed to fetch article: {pmid}\")\n",
    "    return result\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search_by_protein(protein_name: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles about a specific protein.\"\"\"\n",
    "    query = f\"{protein_name}[Title/Abstract] AND protein[Title/Abstract]\"\n",
    "    logger.info(f\"Searching PubMed for protein: {protein_name}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results for protein: {protein_name}\")\n",
    "    return results\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search_by_disease(disease_name: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles about a specific disease.\"\"\"\n",
    "    query = f\"{disease_name}[Title/Abstract] AND (disease[Title/Abstract] OR disorder[Title/Abstract] OR condition[Title/Abstract])\"\n",
    "    logger.info(f\"Searching PubMed for disease: {disease_name}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results for disease: {disease_name}\")\n",
    "    return results\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search_by_drug(drug_name: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles about a specific drug.\"\"\"\n",
    "    query = f\"{drug_name}[Title/Abstract] AND (drug[Title/Abstract] OR medication[Title/Abstract] OR compound[Title/Abstract])\"\n",
    "    logger.info(f\"Searching PubMed for drug: {drug_name}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results for drug: {drug_name}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_clinicaltrial.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "from pytrials.client import ClinicalTrials\n",
    "\n",
    "MAX_OUTPUT_CHARS = 20000\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"clinicaltrial_mcp\")\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"clinicaltrial_tools\")\n",
    "    logger.info(\"Clinical Trial MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "ct = ClinicalTrials()\n",
    "\n",
    "# Helper functions\n",
    "def load_csv_file(filename):\n",
    "    \"\"\"Load data from a CSV file\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    return None\n",
    "\n",
    "def format_limited_output(df, max_rows=None, max_chars=MAX_OUTPUT_CHARS):\n",
    "    \"\"\"Format DataFrame output with character limit and metadata\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return \"No data available\"\n",
    "\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # If maximum rows are specified, limit the output rows\n",
    "    if max_rows and max_rows < total_rows:\n",
    "        display_df = df.head(max_rows)\n",
    "        rows_shown = max_rows\n",
    "    else:\n",
    "        display_df = df\n",
    "        rows_shown = total_rows\n",
    "\n",
    "    # Convert to string\n",
    "    output = display_df.to_string()\n",
    "\n",
    "    # If exceeding character limit, truncate\n",
    "    if len(output) > max_chars:\n",
    "        output = output[:max_chars] + \"\\n...[Output truncated]\"\n",
    "\n",
    "    # Add metadata\n",
    "    metadata = f\"\\n\\nData summary: Total {total_rows} records, showing {rows_shown} records.\"\n",
    "\n",
    "    return output + metadata\n",
    "\n",
    "def list_available_csv_files():\n",
    "    \"\"\"List all available CSV files in the current directory\"\"\"\n",
    "    return [f for f in os.listdir(\".\") if f.endswith(\".csv\")]\n",
    "\n",
    "@mcp.tool()\n",
    "def search_clinical_trials_and_save_studies_to_csv(\n",
    "    search_expr: str,\n",
    "    max_studies: int = 10,\n",
    "    save_csv: bool = False,\n",
    "    filename: str = \"search_results.csv\",\n",
    "    fields: list = None,\n",
    ") -> str:\n",
    "    \"\"\"Search for clinical trials using a search expression\"\"\"\n",
    "    try:\n",
    "        # Default fields if none provided\n",
    "        if fields is None:\n",
    "            fields = [\"NCT Number\", \"Conditions\", \"Study Title\", \"Brief Summary\"]\n",
    "\n",
    "        # Get study fields\n",
    "        results = ct.get_study_fields(\n",
    "            search_expr=search_expr, fields=fields, max_studies=max_studies\n",
    "        )\n",
    "\n",
    "        if len(results) > 1:  # Header + data\n",
    "            df = pd.DataFrame.from_records(results[1:], columns=results[0])\n",
    "\n",
    "            # Save to CSV if requested\n",
    "            if save_csv:\n",
    "                csv_filename = filename or f\"search_results_{search_expr.replace('+', '_')}.csv\"\n",
    "                df.to_csv(csv_filename, index=False)\n",
    "                storage_info = f\"Complete results have been saved to file {csv_filename}\"\n",
    "                return f\"Results saved to {csv_filename}\\n\\n{format_limited_output(df)}\\n{storage_info}\"\n",
    "\n",
    "            return format_limited_output(df)\n",
    "        return \"No results found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching clinical trials: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_full_study_details(nct_id: str) -> str:\n",
    "    \"\"\"Get detailed information about a specific clinical trial\"\"\"\n",
    "    try:\n",
    "        study = ct.get_full_studies(search_expr=f\"NCT Number={nct_id}\", max_studies=1)\n",
    "        if len(study) > 1:  # Header + data\n",
    "            df = pd.DataFrame.from_records(study[1:], columns=study[0])\n",
    "            return format_limited_output(df)\n",
    "        return f\"Study with NCT ID {nct_id} not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching study details: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_studies_by_keyword(\n",
    "    keyword: str, max_studies: int = 20, save_csv: bool = False, filename: str = None\n",
    ") -> str:\n",
    "    \"\"\"Get studies related to a specific keyword\"\"\"\n",
    "    try:\n",
    "        fields = [\"NCT Number\", \"Conditions\", \"Study Title\", \"Brief Summary\"]\n",
    "        results = ct.get_study_fields(\n",
    "            search_expr=keyword, fields=fields, max_studies=max_studies\n",
    "        )\n",
    "\n",
    "        if len(results) > 1:  # Header + data\n",
    "            df = pd.DataFrame.from_records(results[1:], columns=results[0])\n",
    "\n",
    "            # Save to CSV if requested\n",
    "            if save_csv:\n",
    "                csv_filename = filename or f\"keyword_results_{keyword.replace(' ', '_')}.csv\"\n",
    "                df.to_csv(csv_filename, index=False)\n",
    "                storage_info = f\"Complete results have been saved to file {csv_filename}\"\n",
    "                return f\"Results saved to {csv_filename}\\n\\n{format_limited_output(df)}\\n{storage_info}\"\n",
    "\n",
    "            return format_limited_output(df)\n",
    "        return f\"No studies found for keyword: {keyword}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching studies by keyword: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_full_studies_and_save(\n",
    "    search_expr: str, max_studies: int = 20, filename: str = \"full_studies.csv\"\n",
    ") -> str:\n",
    "    \"\"\"Get full studies data and save to CSV\"\"\"\n",
    "    try:\n",
    "        # Get full studies\n",
    "        full_studies = ct.get_full_studies(\n",
    "            search_expr=search_expr, max_studies=max_studies\n",
    "        )\n",
    "\n",
    "        if len(full_studies) > 1:  # Header + data\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame.from_records(full_studies[1:], columns=full_studies[0])\n",
    "\n",
    "            # Save to CSV\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "            return f\"Successfully saved {len(df)} full studies to {filename}\"\n",
    "        return \"No results found to save\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving full studies to CSV: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def load_csv_data(filename: str) -> str:\n",
    "    \"\"\"Load and display data from a CSV file\"\"\"\n",
    "    # Ensure the filename ends with .csv\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        filename += \".csv\"\n",
    "\n",
    "    df = load_csv_file(filename)\n",
    "    if df is not None:\n",
    "        return f\"Loaded data from {filename}:\\n\\n{format_limited_output(df)}\"\n",
    "    return f\"CSV file {filename} not found or could not be loaded\"\n",
    "\n",
    "@mcp.tool()\n",
    "def list_saved_csv_files() -> str:\n",
    "    \"\"\"List all available CSV files in the current directory\"\"\"\n",
    "    files = list_available_csv_files()\n",
    "    if files:\n",
    "        return f\"Available CSV files:\\n\\n{chr(10).join(files)}\"\n",
    "    return \"No CSV files available\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCP 클라이언트 설정\n",
    "\n",
    "각 외부 데이터베이스에 대한 MCP 클라이언트를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 서버 파라미터 설정\n",
    "arxiv_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_arxiv.py\"]\n",
    ")\n",
    "\n",
    "chembl_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_chembl.py\"]\n",
    ")\n",
    "\n",
    "pubmed_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_pubmed.py\"]\n",
    ")\n",
    "\n",
    "clinicaltrial_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_clinicaltrial.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 클라이언트 생성\n",
    "arxiv_client = MCPClient(arxiv_server_params)\n",
    "chembl_client = MCPClient(chembl_server_params)\n",
    "pubmed_client = MCPClient(pubmed_server_params)\n",
    "clinicaltrial_client = MCPClient(clinicaltrial_server_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 개별 데이터베이스 에이전트 생성\n",
    "\n",
    "각 MCP 클라이언트를 사용하는 전문 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock 모델 설정\n",
    "model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArXiv 전문 에이전트\n",
    "arxiv_agent = Agent(\n",
    "    name=\"ArXivAgent\",\n",
    "    system_prompt=\"\"\"당신은 arXiv에서 학술 논문을 검색하는 전문가입니다. \n",
    "    사용자의 질문에 따라 관련된 과학 문헌을 찾아 제공하세요.\n",
    "    검색 결과는 논문 제목, 저자, 요약, URL을 포함해야 합니다.\"\"\",\n",
    "    model=model,\n",
    "    tools=[arxiv_client]\n",
    ")\n",
    "\n",
    "# ChEMBL 전문 에이전트\n",
    "chembl_agent = Agent(\n",
    "    name=\"ChEMBLAgent\",\n",
    "    system_prompt=\"\"\"당신은 ChEMBL 데이터베이스에서 화학 화합물 정보를 검색하는 전문가입니다.\n",
    "    화합물의 구조, 특성, 생물학적 활성 데이터를 제공하세요.\n",
    "    검색 결과는 ChEMBL ID, 화합물명, 분자식, 분자량 등을 포함해야 합니다.\"\"\",\n",
    "    model=model,\n",
    "    tools=[chembl_client]\n",
    ")\n",
    "\n",
    "# PubMed 전문 에이전트\n",
    "pubmed_agent = Agent(\n",
    "    name=\"PubMedAgent\",\n",
    "    system_prompt=\"\"\"당신은 PubMed에서 생의학 문헌을 검색하는 전문가입니다.\n",
    "    의학, 생물학, 생명과학 관련 연구 논문을 찾아 제공하세요.\n",
    "    검색 결과는 PMID, 논문 제목, 저자, 출판 정보를 포함해야 합니다.\"\"\",\n",
    "    model=model,\n",
    "    tools=[pubmed_client]\n",
    ")\n",
    "\n",
    "# ClinicalTrials 전문 에이전트\n",
    "clinicaltrial_agent = Agent(\n",
    "    name=\"ClinicalTrialAgent\",\n",
    "    system_prompt=\"\"\"당신은 ClinicalTrials.gov에서 임상시험 정보를 검색하는 전문가입니다.\n",
    "    진행 중이거나 완료된 임상 연구에 대한 정보를 제공하세요.\n",
    "    검색 결과는 NCT ID, 연구 제목, 상태, 단계, 조건을 포함해야 합니다.\"\"\",\n",
    "    model=model,\n",
    "    tools=[clinicaltrial_client]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 메인 연구 에이전트 생성 (Agent-as-Tool 패턴)\n",
    "\n",
    "개별 데이터베이스 에이전트들을 도구로 사용하는 통합 연구 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통합 연구 에이전트 생성\n",
    "research_agent = Agent(\n",
    "    name=\"생명과학연구에이전트\",\n",
    "    system_prompt=\"\"\"당신은 종합적인 생명과학 연구 어시스턴트입니다. \n",
    "    다음과 같은 전문 데이터베이스 에이전트들을 도구로 사용할 수 있습니다:\n",
    "    \n",
    "    - ArXivAgent: 학술 논문 및 프리프린트 검색\n",
    "    - ChEMBLAgent: 화학 화합물 데이터 검색\n",
    "    - PubMedAgent: 생의학 문헌 검색\n",
    "    - ClinicalTrialAgent: 임상시험 정보 검색\n",
    "    \n",
    "    사용자의 연구 질문에 따라 적절한 에이전트를 선택하여 사용하세요.\n",
    "    포괄적인 답변을 위해 여러 에이전트를 조합하여 사용할 수 있습니다.\n",
    "    각 에이전트의 검색 결과를 종합하여 유용한 정보를 제공하세요.\"\"\",\n",
    "    model=model,\n",
    "    tools=[\n",
    "        arxiv_agent,\n",
    "        chembl_agent,\n",
    "        pubmed_agent,\n",
    "        clinicaltrial_agent\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 테스트 및 실습\n",
    "\n",
    "### 6.1 개별 에이전트 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArXiv 에이전트 테스트\n",
    "print(\"=== ArXiv 에이전트 테스트 ===\")\n",
    "arxiv_result = arxiv_agent(\"machine learning drug discovery\")\n",
    "print(arxiv_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChEMBL 에이전트 테스트\n",
    "print(\"=== ChEMBL 에이전트 테스트 ===\")\n",
    "chembl_result = chembl_agent(\"aspirin\")\n",
    "print(chembl_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubMed 에이전트 테스트\n",
    "print(\"=== PubMed 에이전트 테스트 ===\")\n",
    "pubmed_result = pubmed_agent(\"COVID-19 치료법\")\n",
    "print(pubmed_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClinicalTrial 에이전트 테스트\n",
    "print(\"=== ClinicalTrial 에이전트 테스트 ===\")\n",
    "ct_result = clinicaltrial_agent(\"당뇨병\")\n",
    "print(ct_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 통합 연구 에이전트 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합적인 연구 질문 테스트\n",
    "research_query = \"심혈관 질환 예방을 위한 아스피린의 효과에 대한 최신 연구 동향을 알려주세요. 관련 논문, 화합물 정보, 임상시험 데이터를 포함해서 답변해주세요.\"\n",
    "\n",
    "print(f\"🔬 연구 질문: {research_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "research_result = research_agent(research_query)\n",
    "print(f\"\\n📊 연구 결과:\\n{research_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 대화형 연구 세션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화형 연구 함수\n",
    "async def interactive_research(query: str):\n",
    "    \"\"\"대화형 연구 질문 처리 함수\"\"\"\n",
    "    print(f\"🔬 연구 질문: {query}\\n\")\n",
    "    print(\"처리 중...\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = research_agent(query)\n",
    "        print(f\"📊 연구 결과:\\n{result}\\n\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 사용 예시 (아래 셀에서 실행)\n",
    "# \"CRISPR 유전자 편집 기술의 최신 발전사항은?\"\n",
    "# \"알츠하이머병 치료제 개발 현황을 알려주세요\"\n",
    "# \"mRNA 백신 기술에 대한 최신 연구는?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 질문들 실행\n",
    "example_queries = [\n",
    "    \"CRISPR 유전자 편집 기술의 최신 발전사항은?\",\n",
    "    \"알츠하이머병 치료제 개발 현황을 알려주세요\",\n",
    "    \"mRNA 백신 기술에 대한 최신 연구는?\"\n",
    "]\n",
    "\n",
    "for query in example_queries:\n",
    "    await interactive_research(query)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
